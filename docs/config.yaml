train:
    total_steps: 3e6
    # total_episodes: 100000
    warmup_steps: 10000
    max_steps_per_ep: 1000
    batch_size: 256
    step_update_interval: 1
    step_store_interval: 4e3
    step_summary_interval: 5e3
    average_factor: 1
    data:
      # train_method == TD (for TD methods where replay_buffer will be used) or tran_method == policy_optimization where
      # no replay buffer will be used, but rather trajectory histories
      train_method: TD
      capacity: 1e6
      # 'custom', 'cpprb', default is custom
      type: cpprb

#evaluation:
#  total_steps: 1000
#  # total_episodes: 10
#  max_steps_per_ep: 200
#  num_top_options: 5
#  # Evaluation interval counted in number of updates to the agent during the train procedure
#  update_eval_interval: 2000
#  average_factor: 3

